{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekstaxy/Mix_Wave_U_Net/blob/main/Wave_U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "P2UARqnv97yc"
      },
      "source": [
        "# Wave-U-Net Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXrTnLugA7Jf",
        "outputId": "33326a79-a958-4dee-f72d-c040ec824001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset contents: ['ENST-drums-public']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Extract your zip file (update the path to your actual zip file)\n",
        "zip_path = '/content/drive/MyDrive/ENST-drums-audio.zip'  # Change this to your zip path\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Set your dataset directory (update based on extracted folder name)\n",
        "dataset_dir = '/content/ENST-drums-audio'  # Change this to match your extracted folder\n",
        "\n",
        "# Verify it worked\n",
        "print(\"Dataset contents:\", os.listdir(dataset_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "F6sduCFU97yd"
      },
      "source": [
        "## Import Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrtXnLgU97ye",
        "outputId": "a957fa24-abb3-4ca0-a0f5-212713892239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyloudnorm\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm) (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm) (2.0.2)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm) (1.0.0)\n",
            "Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: pyloudnorm\n",
            "Successfully installed pyloudnorm-0.1.1\n",
            "Collecting auraloss\n",
            "  Downloading auraloss-0.4.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from auraloss) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from auraloss) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->auraloss) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->auraloss) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->auraloss) (3.0.2)\n",
            "Downloading auraloss-0.4.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: auraloss\n",
            "Successfully installed auraloss-0.4.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "\n",
        "!pip install pyloudnorm\n",
        "import pyloudnorm\n",
        "!pip install auraloss\n",
        "import auraloss\n",
        "\n",
        "import warnings\n",
        "# Add this at the top of your code to suppress these specific warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Possible clipped samples in output.\")\n",
        "\n",
        "# !pip install git+https://github.com/csteinmetz1/automix-toolkit\n",
        "# import automix.utils\n",
        "# import automix.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9mCA3g6o97yf"
      },
      "source": [
        "## Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LBhnDw6y97yf"
      },
      "outputs": [],
      "source": [
        "class ENST_Drumset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir: str,\n",
        "        sr: float,\n",
        "        length: int,\n",
        "        drummers: List[int] = [1, 2],\n",
        "        track_names: List[str] = [\n",
        "            \"kick\",\n",
        "            \"snare\",\n",
        "            \"hihat\",\n",
        "            \"overhead_L\",\n",
        "            \"overhead_R\",\n",
        "            \"tom_1\",\n",
        "            \"tom_2\",\n",
        "            \"tom_3\"\n",
        "        ],\n",
        "        indices: Tuple[int, int] = [0, 1],\n",
        "        wet_mix: bool = False,\n",
        "        hits: bool = False,\n",
        "        num_examples_per_epoch: int = 1000,\n",
        "        seed: int = 42\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.length = length\n",
        "        self.sr = sr\n",
        "        self.drummers = drummers\n",
        "        self.track_names = track_names\n",
        "        self.indices = indices\n",
        "        self.wet_mix = wet_mix\n",
        "        self.hits = hits\n",
        "        self.num_examples_per_epoch = num_examples_per_epoch\n",
        "        self.seed = seed\n",
        "        self.max_num_tracks = 8\n",
        "        self.mix_filepaths = []\n",
        "\n",
        "        if not os.path.isdir(root_dir):\n",
        "            raise FileNotFoundError(f\"找不到指定的音訊檔案目錄：{root_dir}\")\n",
        "\n",
        "        for drummer in drummers:\n",
        "            search_path = os.path.join(\n",
        "                root_dir,\n",
        "                f\"drummer_{drummer}\",\n",
        "                \"audio\",\n",
        "                \"wet_mix\" if wet_mix else \"dry_mix\",\n",
        "                \"*.wav\",\n",
        "            )\n",
        "            self.mix_filepaths += glob.glob(search_path)\n",
        "\n",
        "        # remove any mixes that is shorter than required length\n",
        "        self.mix_filepaths = [\n",
        "            fp\n",
        "            for fp in self.mix_filepaths\n",
        "            if torchaudio.info(fp).num_frames > self.length\n",
        "        ]\n",
        "\n",
        "        # remove any mixes that have \"norm\" in the filename\n",
        "        self.mix_filepaths = [fp for fp in self.mix_filepaths if not \"norm\" in fp]\n",
        "\n",
        "        # remove any mixes that are just hits\n",
        "        if not self.hits:\n",
        "            self.mix_filepaths = [fp for fp in self.mix_filepaths if \"hits\" not in fp]\n",
        "\n",
        "        random.Random(seed).shuffle(self.mix_filepaths)\n",
        "        self.mix_filepaths = self.mix_filepaths[indices[0] : indices[1]]\n",
        "\n",
        "        if len(self.mix_filepaths) < 1:\n",
        "            raise RuntimeError(f\"No files found in {self.root_dir}.\")\n",
        "        else:\n",
        "            print(f\"Found {len(self.mix_filepaths)} examples from drummers: {drummers}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mix_filepaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mix_idx = idx\n",
        "        if mix_idx >= len(self.mix_filepaths):\n",
        "            raise IndexError(f\"idx : {mix_idx} out of sequence length : {len(self.mix_filepaths)}\")\n",
        "        mix_filepath = self.mix_filepaths[idx]\n",
        "        example_id = os.path.basename(mix_filepath)\n",
        "        drummer_id = os.path.normpath(mix_filepath).split(os.path.sep)[-4]\n",
        "\n",
        "        md = torchaudio.info(mix_filepath)  # check length\n",
        "\n",
        "        silent = True\n",
        "        while silent:\n",
        "            # get random offset\n",
        "            offset = offset = np.random.randint(0, md.num_frames - self.length - 1)\n",
        "\n",
        "            y, sr = torchaudio.load(\n",
        "                uri = mix_filepath,\n",
        "                frame_offset = offset,\n",
        "                num_frames = self.length,\n",
        "                normalize = True,\n",
        "            )\n",
        "            energy = (y**2).mean()\n",
        "            if energy > 1e-8:\n",
        "                silent = False\n",
        "\n",
        "        y = y.float()\n",
        "        y /= y.abs().max().clamp(1e-8)  # peak normalize\n",
        "        # y_numpy = y.squeeze().numpy()\n",
        "        # if y_numpy.ndim > 1:\n",
        "        #     y_numpy = y_numpy.T\n",
        "        # meter = pyloudnorm.Meter(self.sr)  # create BS.1770 meter\n",
        "        # y_loudness = meter.integrated_loudness(y_numpy)\n",
        "\n",
        "        # y_normalized_numpy = pyloudnorm.normalize.loudness(y_numpy, y_loudness, -24.0)\n",
        "        # y_normalized_numpy = np.clip(y_normalized_numpy, -1.0, 1.0)\n",
        "        # if y_normalized_numpy.ndim > 1:\n",
        "        #     y_normalized_numpy = y_normalized_numpy.T\n",
        "        # y = torch.from_numpy(y_normalized_numpy)  # Add channel dim\n",
        "\n",
        "        x = torch.zeros((self.max_num_tracks, self.length))\n",
        "        pad = [True] * self.max_num_tracks  # note which tracks are empty\n",
        "\n",
        "        for tidx, track_name in enumerate(self.track_names):\n",
        "            track_path = os.path.join(\n",
        "                self.root_dir,\n",
        "                drummer_id,\n",
        "                \"audio\",\n",
        "                track_name,\n",
        "                example_id\n",
        "            )\n",
        "            if os.path.isfile(track_path):\n",
        "                x_s, sr = torchaudio.load(\n",
        "                    uri = track_path,\n",
        "                    frame_offset = offset,\n",
        "                    num_frames = self.length\n",
        "                )\n",
        "                # # Convert the PyTorch tensor to a NumPy array\n",
        "                # x_s_numpy = x_s.squeeze().numpy()\n",
        "                # # Transpose for pyloudnorm: (channels, samples) -> (samples, channels)\n",
        "                # if x_s_numpy.ndim > 1:\n",
        "                #     x_s_numpy = x_s_numpy.T\n",
        "                # # Create a loudness meter instance from pyloudnorm\n",
        "                # meter = pyloudnorm.Meter(self.sr)  # create BS.1770 meter\n",
        "                # # Measure the loudness of the NumPy array\n",
        "                # x_loudness = meter.integrated_loudness(x_s_numpy)\n",
        "                # # Normalize the NumPy array to -24 LUFS\n",
        "\n",
        "                # x_s_normalized_numpy = pyloudnorm.normalize.loudness(x_s_numpy, x_loudness, -24.0)\n",
        "                # x_s_normalized_numpy = np.clip(x_s_normalized_numpy, -1.0, 1.0)\n",
        "                # # Transpose back for PyTorch: (samples, channels) -> (channels, samples)\n",
        "                # if x_s_normalized_numpy.ndim > 1:\n",
        "                #     x_s_normalized_numpy = x_s_normalized_numpy.T\n",
        "                # # Convert the normalized NumPy array back to a PyTorch tensor\n",
        "                # x_s = torch.from_numpy(x_s_normalized_numpy)\n",
        "                x_s /= x_s.abs().max().clamp(1e-6)\n",
        "                x_s *= 10 ** (-12 / 20.0)\n",
        "                x[tidx, :] = x_s\n",
        "                pad[tidx] = False\n",
        "\n",
        "        return x, y, torch.tensor(pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "e0tslson97yg"
      },
      "source": [
        "## Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z2T38_ei97yg"
      },
      "outputs": [],
      "source": [
        "class DownSampling(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channel_in: int,\n",
        "        channel_out: int,\n",
        "        kernel_size: int = 15\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 確保 kernel_size不可被 2 整除，padding = 'same'\n",
        "        assert kernel_size % 2 != 0\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            channel_in,\n",
        "            channel_out,\n",
        "            kernel_size = kernel_size,\n",
        "            padding = padding\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm1d(channel_out)\n",
        "        self.prelu = nn.PReLU(channel_out)\n",
        "        self.conv2 = nn.Conv1d(\n",
        "            channel_out,\n",
        "            channel_out,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = 2,\n",
        "            padding = padding\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu(x)\n",
        "        x_ds = self.conv2(x)\n",
        "        return x_ds, x\n",
        "\n",
        "class UpSampling(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channel_in: int,\n",
        "        channel_out: int,\n",
        "        kernel_size: int = 5,\n",
        "        skip: str = 'add'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert kernel_size % 2 != 0\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        self.skip = skip\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            channel_in,\n",
        "            channel_out,\n",
        "            kernel_size = kernel_size,\n",
        "            padding = padding\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm1d(channel_out)\n",
        "        self.prelu = nn.PReLU(channel_out)\n",
        "        self.upsampling = nn.Upsample(scale_factor = 2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip: torch.Tensor):\n",
        "        x = self.upsampling(x)\n",
        "\n",
        "        if self.skip == 'add':\n",
        "            x = x + skip\n",
        "        elif self.skip == 'concat':\n",
        "            x = torch.cat((x, skip), dim = 1)\n",
        "        elif self.skip == 'none':\n",
        "            pass\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MixWaveUNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_inputs: int = 8,\n",
        "        n_outputs: int = 2,\n",
        "        ds_kernel: int = 15,    # 13\n",
        "        us_kernel: int = 5,     # 13\n",
        "        out_kernel: int = 5,\n",
        "        layers: int = 10,\n",
        "        channel_growth: int = 24,\n",
        "        skip: str = 'concat'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        for n in range(layers):\n",
        "            if n == 0:\n",
        "                channel_in = n_inputs\n",
        "                channel_out = channel_growth\n",
        "            else:\n",
        "                channel_in = channel_out\n",
        "                channel_out = channel_in + channel_growth\n",
        "\n",
        "            self.encoder.append(DownSampling(channel_in, channel_out, kernel_size = ds_kernel))\n",
        "\n",
        "        # print(\"Encoder Structure\")\n",
        "        # print(self.encoder)\n",
        "\n",
        "        self.embedding = nn.Conv1d(channel_out, channel_out, kernel_size = 1)\n",
        "        # print(\"Embedder Structure\")\n",
        "        # print(self.embedding)\n",
        "\n",
        "        self.decoder = nn.ModuleList()\n",
        "        for n in range(layers, 0, -1):\n",
        "            channel_in = channel_out\n",
        "            channel_out = channel_in - channel_growth\n",
        "\n",
        "            if channel_out < channel_growth:\n",
        "                channel_out = channel_growth\n",
        "\n",
        "            if skip == 'concat':\n",
        "                channel_in *= 2\n",
        "\n",
        "            self.decoder.append(UpSampling(channel_in, channel_out, kernel_size=us_kernel, skip=skip))\n",
        "\n",
        "        # print(\"Decoder Structure\")\n",
        "        # print(self.decoder)\n",
        "\n",
        "        self.output_conv = nn.Conv1d(channel_out + n_inputs, n_outputs, kernel_size = out_kernel, padding = out_kernel // 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        skips = []\n",
        "\n",
        "        for enc in self.encoder:\n",
        "            x, skip = enc(x)\n",
        "            skips.append(skip)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        for dec in self.decoder:\n",
        "            skip = skips.pop()\n",
        "            x = dec(x, skip)\n",
        "\n",
        "        x = torch.cat((x_in, x), dim = 1)\n",
        "        y = self.output_conv(x)\n",
        "\n",
        "        return y, torch.zeros(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7WCX5xyS97yg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_07JruI97yg",
        "outputId": "8be8e46b-da0b-495a-e437-da928d772db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixWaveUNet(\n",
              "  (encoder): ModuleList(\n",
              "    (0): DownSampling(\n",
              "      (conv1): Conv1d(8, 24, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=24)\n",
              "      (conv2): Conv1d(24, 24, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (1): DownSampling(\n",
              "      (conv1): Conv1d(24, 48, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=48)\n",
              "      (conv2): Conv1d(48, 48, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (2): DownSampling(\n",
              "      (conv1): Conv1d(48, 72, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=72)\n",
              "      (conv2): Conv1d(72, 72, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (3): DownSampling(\n",
              "      (conv1): Conv1d(72, 96, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=96)\n",
              "      (conv2): Conv1d(96, 96, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (4): DownSampling(\n",
              "      (conv1): Conv1d(96, 120, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=120)\n",
              "      (conv2): Conv1d(120, 120, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (5): DownSampling(\n",
              "      (conv1): Conv1d(120, 144, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=144)\n",
              "      (conv2): Conv1d(144, 144, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (6): DownSampling(\n",
              "      (conv1): Conv1d(144, 168, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=168)\n",
              "      (conv2): Conv1d(168, 168, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (7): DownSampling(\n",
              "      (conv1): Conv1d(168, 192, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=192)\n",
              "      (conv2): Conv1d(192, 192, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (8): DownSampling(\n",
              "      (conv1): Conv1d(192, 216, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=216)\n",
              "      (conv2): Conv1d(216, 216, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "    (9): DownSampling(\n",
              "      (conv1): Conv1d(216, 240, kernel_size=(15,), stride=(1,), padding=(7,))\n",
              "      (batchnorm): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=240)\n",
              "      (conv2): Conv1d(240, 240, kernel_size=(15,), stride=(2,), padding=(7,))\n",
              "    )\n",
              "  )\n",
              "  (embedding): Conv1d(240, 240, kernel_size=(1,), stride=(1,))\n",
              "  (decoder): ModuleList(\n",
              "    (0): UpSampling(\n",
              "      (conv): Conv1d(480, 216, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=216)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (1): UpSampling(\n",
              "      (conv): Conv1d(432, 192, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=192)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (2): UpSampling(\n",
              "      (conv): Conv1d(384, 168, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=168)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (3): UpSampling(\n",
              "      (conv): Conv1d(336, 144, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=144)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (4): UpSampling(\n",
              "      (conv): Conv1d(288, 120, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=120)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (5): UpSampling(\n",
              "      (conv): Conv1d(240, 96, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=96)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (6): UpSampling(\n",
              "      (conv): Conv1d(192, 72, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=72)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (7): UpSampling(\n",
              "      (conv): Conv1d(144, 48, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=48)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (8): UpSampling(\n",
              "      (conv): Conv1d(96, 24, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=24)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "    (9): UpSampling(\n",
              "      (conv): Conv1d(48, 24, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (batchnorm): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (prelu): PReLU(num_parameters=24)\n",
              "      (upsampling): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    )\n",
              "  )\n",
              "  (output_conv): Conv1d(32, 2, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset_dir = '/content/ENST-drums-audio/ENST-drums-public'\n",
        "dataset_name = 'ESTN_Drum_Dataset'\n",
        "sample_rate = 44100\n",
        "train_length = 262144\n",
        "val_length = 262144\n",
        "test_length = 262144\n",
        "output_length = 262144\n",
        "max_num_track = 8\n",
        "wet_mix = False\n",
        "\n",
        "batch_size = 16\n",
        "lr = 0.001\n",
        "max_epochs = 50\n",
        "patient = 5\n",
        "num_workers = 1\n",
        "\n",
        "model = MixWaveUNet()\n",
        "loss_function = auraloss.freq.SumAndDifferenceSTFTLoss(\n",
        "    fft_sizes = [512, 1024, 2048, 4096],  # Multiple scales for multi-resolution\n",
        "    hop_sizes = [128, 256, 512, 1024],    # 25% overlap (hop = fft_size/4)\n",
        "    win_lengths = [512, 1024, 2048, 4096], # Same as fft_sizes for full window\n",
        "    window = \"hann_window\",\n",
        "    w_sum = 1.0,\n",
        "    w_diff = 1.0,\n",
        "    output = \"loss\",\n",
        "    sample_rate = sample_rate,\n",
        ")\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "model_save_path = '/content/ckpt'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSt1IAzr97yh",
        "outputId": "3b7210dc-6eb3-4985-c650-17d16cc2ffbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2366015322.py:56: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  if torchaudio.info(fp).num_frames > self.length\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:20: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:27: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  return AudioMetaData(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 168 examples from drummers: [1, 2, 3]\n",
            "Found 21 examples from drummers: [1, 2, 3]\n",
            "Found 21 examples from drummers: [1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = ENST_Drumset(\n",
        "    root_dir = dataset_dir,\n",
        "    sr = sample_rate,\n",
        "    length = train_length,\n",
        "    drummers = [1, 2, 3],\n",
        "    indices = [0, 168],\n",
        "    num_examples_per_epoch = 1000,\n",
        "    wet_mix = wet_mix,\n",
        ")\n",
        "val_dataset = ENST_Drumset(\n",
        "    root_dir = dataset_dir,\n",
        "    sr = sample_rate,\n",
        "    length = val_length,\n",
        "    drummers = [1, 2, 3],\n",
        "    indices = [168, 189],\n",
        "    num_examples_per_epoch = 1000,\n",
        "    wet_mix = wet_mix,\n",
        ")\n",
        "test_dataset = ENST_Drumset(\n",
        "    root_dir = dataset_dir,\n",
        "    sr = sample_rate,\n",
        "    length = test_length,\n",
        "    drummers = [1, 2, 3],\n",
        "    indices = [189, 210],\n",
        "    num_examples_per_epoch = 1000,\n",
        "    wet_mix = wet_mix,\n",
        ")\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    persistent_workers=True,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers=1,\n",
        "    persistent_workers=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhSk03be97yh",
        "outputId": "f7d8ec11-a68c-423d-9d2e-76ed2d7b18b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2366015322.py:86: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  md = torchaudio.info(mix_filepath)  # check length\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 4.8441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2366015322.py:86: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  md = torchaudio.info(mix_filepath)  # check length\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50\n",
            "Train Loss: 1.9976\n",
            "Epoch 3/50\n",
            "Train Loss: 1.6886\n",
            "Epoch 4/50\n",
            "Train Loss: 1.6050\n",
            "Epoch 5/50\n",
            "Train Loss: 1.5370\n",
            "Epoch 6/50\n",
            "Train Loss: 1.4700\n"
          ]
        }
      ],
      "source": [
        "best_val_lost = 10000000\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # idx, x, y = next(iter(train_dataloader))\n",
        "    # print(idx, x, y)\n",
        "\n",
        "    for idx, data in enumerate(train_dataloader):\n",
        "        x_batch, y_batch, pad = data\n",
        "        # x_batch = x_batch.unsqueeze(1)\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(x_batch)\n",
        "        loss = loss_function(output, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{max_epochs}\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Model Evaluation\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(val_dataloader):\n",
        "            x_batch, y_batch, pad = data\n",
        "            # x_batch = x_batch.unsqueeze(1)\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            output, _ = model(x_batch)\n",
        "            output = output.clamp(min=-1.0, max=1.0)\n",
        "            loss = loss_function(output, y_batch)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "    if best_val_lost > avg_val_loss:\n",
        "        best_val_lost = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patient:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs due to no improvement for {patient} epochs.\")\n",
        "            break\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SwSrRwiN97yh"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8I6gv5t97yh"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # Use a batch size of 1 for evaluation\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    persistent_workers=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IyCLJMkO97yh"
      },
      "outputs": [],
      "source": [
        "output_dir = './output_audio'\n",
        "if not os.path.isdir(output_dir):\n",
        "    os.makedirs(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWqH2pA_97yi"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Loop through the test dataset\n",
        "with torch.no_grad():\n",
        "    for idx, data in enumerate(test_dataloader):\n",
        "        # Unpack the data\n",
        "        x_batch, y_batch, pad = data\n",
        "\n",
        "        # Move tensors to the correct device\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # Pass the mix through the trained model\n",
        "        predicted_output, _ = model(x_batch)\n",
        "        predicted_output = predicted_output.clamp(min=-1.0, max=1.0)\n",
        "\n",
        "        # Move the tensors to the CPU for saving\n",
        "        x_batch = x_batch.cpu()\n",
        "        y_batch = y_batch.cpu()\n",
        "        predicted_output = predicted_output.cpu()\n",
        "\n",
        "        print(f\"Saving and playing example {idx+1}...\")\n",
        "\n",
        "        # Save the original mix (y_batch)\n",
        "        original_mix_path = os.path.join(output_dir, f\"example_{idx+1}_original_mix.wav\")\n",
        "        torchaudio.save(\n",
        "            original_mix_path,\n",
        "            y_batch.squeeze(0),\n",
        "            sample_rate,\n",
        "        )\n",
        "\n",
        "        # Save the model's two-channel output as a single stereo file\n",
        "        separated_mix_path = os.path.join(output_dir, f\"example_{idx+1}_separated_mix.wav\")\n",
        "        torchaudio.save(\n",
        "            separated_mix_path,\n",
        "            predicted_output.squeeze(0),\n",
        "            sample_rate,\n",
        "        )\n",
        "\n",
        "        # Display the audio players for this example\n",
        "        print(\"Original Mix:\")\n",
        "        display(Audio(original_mix_path))\n",
        "\n",
        "        print(\"Separated Mix:\")\n",
        "        display(Audio(separated_mix_path))\n",
        "\n",
        "print(\"\\nEvaluation complete. All audio files saved to the 'output_audio' directory.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8153873,
          "sourceId": 12887698,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}